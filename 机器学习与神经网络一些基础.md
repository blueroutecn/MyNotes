# 一些零碎记录

### 区分生成模型与判别模型

* 生成模型从数据学习联合概率密度分布P(X,Y)，然后再求出条件概率分布P(Y|X)的模型；
* 判别模型直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型。
* 可参考资料：吴岸城《神经网络与机器学习》P104


### 关于矩阵乘法

* 不满足交换律

### 3Blue1Brown 神经网络Part1:神经网络的结果

* https://www.bilibili.com/video/av15532370

* 前一层每一个神经元都将通过权重w对下一层的每个神经元进行“激活”(eg. 例如第0层的每个神经元a0,i对第2层第一个神经元a1,1的激活为:a1,1 = a0,1w0,1 + a0,2w0,2 + ... a0,N*w0,N)。由于计算后结果相差可能很大，因此通过激活函数激活函数(eg.sigomid)将前一层神经元对下一层某个神经元的影响值(eg. a1,1 = a0,1w0,1 + a0,2w0,2 + ... a0,N*w0,N)进行映射，从而决定某个神经元是否被激活。但为了防止任意映射的出现，需要添加一个偏置值bias;假设激活函数是f(x)，则最终第一层全部神经元对第二层第一个神经元的激活结果为f( a0,1w0,1 + a0,2w0,2 + ... a0,N*w0,N + b).


* 激活函数(eg.sigomid)，将前一层神经元对下一层某个神经元的权重与bias集散结果映射为0-1之间的一个数，从而决定某个神经元是否被激活。从而决定某个神经元是否被激活
bias值使得不能随意进行映射。早期使用的激活函数是sigmoid,后期又使用ReLu函数等。



### 3Blue1Brown 神经网络Part2:梯度下降法

* https://www.bilibili.com/video/av16144388 

* 神经网络的核心是通计算梯度找到一个局部最小(从而使得使得代价函数值最小)
代价函数cost function实际是关于权重w和偏置b的函数，通过梯度下降法寻找当w,b时何值时，能够在cost functino中找到一个局部最小值，使得模型与实际数据件的误差最小。在寻求局部最小时求关于w,b的梯度值，梯度代表了在(w,b)点出函数增长最快的方向，梯度的负方向就是函数值下降最快的方向，由此来寻找局部最小值，即梯度下降法。

* 函数的梯度给出了函数增长最快的方向，反之，梯度的负方向就是下降最快的方向。

* Twitter:Michale_ninlsen  http://neuralnetworksanddeeplearning.com/

* 知乎：张小磊

* B站评论中的一篇文章https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit

### 3Blue1Brown 神经网络Part3-4:Back Propagation逆向传播算法

* https://www.bilibili.com/video/av16577449?p=1  视频还是有些没看懂

* 如何正确理解BP算法：https://www.zhihu.com/question/27239198

* 《神经网络与深度学习》P84: Back Propagation,又称 Error Back Propagation误差反向传播；BP算法的本质是**信号正向传播**和**误差反向传播**
    * 信号正向传播：正向传播时，输入样本从输出样本传入，经各隐藏层逐层处理后传向输出层。若此时的实际输出与期望的输出不符，则转入无法的反向传播阶段；

    * 误差逆向传播:将输出以某种形式通过隐藏层向输入层逐层反转，并将误差分摊给各层的**所有单元**，从而获得各层的误差信号，作为修改各个单元权值的依据。

* Part4中的计算过程可以参见西瓜书P102-P103。且西瓜书提供了有关学习率的内容。

### 概率

* https://www.zhihu.com/question/24261751
https://zhuanlan.zhihu.com/p/25768606


* 先验概率，后验概率，条件概率，似然概率，条件概率，似然概率与后验概率区别,

先验概率prior probability

后验概率posterior probability：由结果估计原因的概率分布(或看某个因素对结果的影响程度？？)

条件概率：一个事件发生后另一个事件发生的概率

似然估计likelihood：先确定原因，根据原因来估计结果的概率分布

* 全概率

* 最大似然估计MLE,最大后验概率估计MAP， 贝叶斯估计

    * MLE与MAP的区别与联系(提到了 经验风险最小化 和 结构风险最小化) https://zhuanlan.zhihu.com/p/36606635

* 贝叶斯估计

   * [神奇的贝叶斯定理](https://www.jianshu.com/p/283154606af5)

   * [极大似然估计 与 贝叶斯估计](https://blog.csdn.net/liu1194397014/article/details/52766760)

   * [详解贝叶斯估计,最大似然估计和最大后验概率估计](https://blog.csdn.net/u011508640/article/details/72815981)

   * [常见的参数估计方法](https://www.jianshu.com/p/f4ea000892ca)

   * [三种参数估计方法 - MLE,MAP和贝叶斯估计](https://blog.csdn.net/leo_xu06/article/details/51222215)

* 极大似然估计Maximum Likelyhood Estimation

    * https://zhuanlan.zhihu.com/p/29972515

    * https://zhuanlan.zhihu.com/p/36824006

    * 极大似然估计是一种参数估计的方法。它要解决的问题是：对给定一组数据和一个参数待定的模型，如何确定模型的参数，使得这个确定参数后的模型在所有模型中产生已知数据的概率最大。 例如，抛硬币4次，两次正面两次反面，结果分别是正，正，反，反，若出现正反的概率分别是p和1-p,则此次事件X出现的概率是P = p^2 * (1-p)^2。最大似然估计要做的，就是算出当p为何止值时，此现象最有可能发生，即事件出现的概率最大，即求P的最大值。通过计算，得到当p=0.5时，P有最大值。

   * 理解补充：MLE求的是当p=x时事件X发送的概率最大，即求p=x使得P(X|p)最大。
    
    * 个人理解：最大似然估计是求参数α使得当前现象/事件出现的概率最大，即最大化该事件/现象发生的可能性(因此先列出当前现象出现的概率值P，然后求P的最值)

    * 问题：MLE估计不会把先验知识考虑进去，而且很容易造成过拟合现象

* 最大后验概率Maximum A Posteriori Estimation，MAP

    * 最大后验估计与极大似然估计最大的区别是，**它考虑了参数本身的分布，也就是先验分布**。 MAP优化的是一个后验概率，即给定了观测值后使概率最大。

    * MAP其他内容可参考此文https://www.cnblogs.com/sylvanas2012/p/5058065.html

    * https://blog.csdn.net/klqulei123/article/details/52781643

    * [BL, MAP, ML](http://www.360doc.com/content/16/0510/11/29157075_557802881.shtml)

----

MLP 与神经网络区别

随机梯度上升/下降 与 一般所说的 梯度上升/下降 什么区别？？



反向传播算法