# 一些零碎记录

### 区分生成模型与判别模型

* 生成模型从数据学习联合概率密度分布P(X,Y)，然后再求出条件概率分布P(Y|X)的模型；
* 判别模型直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型。
* 可参考资料：吴岸城《神经网络与机器学习》P104


### 关于矩阵乘法

* 不满足交换律

### 3Blue1Brown 神经网络Part1:神经网络的结果

* https://www.bilibili.com/video/av15532370

* 前一层每一个神经元都将通过权重w对下一层的每个神经元进行“激活”(eg. 例如第0层的每个神经元a0,i对第2层第一个神经元a1,1的激活为:a1,1 = a0,1w0,1 + a0,2w0,2 + ... a0,N*w0,N)。由于计算后结果相差可能很大，因此通过激活函数激活函数(eg.sigomid)将前一层神经元对下一层某个神经元的影响值(eg. a1,1 = a0,1w0,1 + a0,2w0,2 + ... a0,N*w0,N)进行映射，从而决定某个神经元是否被激活。但为了防止任意映射的出现，需要添加一个偏置值bias;假设激活函数是f(x)，则最终第一层全部神经元对第二层第一个神经元的激活结果为f( a0,1w0,1 + a0,2w0,2 + ... a0,N*w0,N + b).

* 激活函数(eg.sigomid)，将前一层神经元对下一层某个神经元的权重与bias集散结果映射为0-1之间的一个数，从而决定某个神经元是否被激活。从而决定某个神经元是否被激活
bias值使得不能随意进行映射。早期使用的激活函数是sigmoid,后期又使用ReLu函数等。
ReLu


### 3Blue1Brown 神经网络Part2:梯度下降法

* https://www.bilibili.com/video/av16144388 

* 神经网络的核心是通计算梯度找到一个局部最小(从而使得使得代价函数值最小)
代价函数cost function实际是关于权重w和偏置b的函数，通过梯度下降法寻找当w,b时何值时，能够在cost functino中找到一个局部最小值，使得模型与实际数据件的误差最小。在寻求局部最小时求关于w,b的梯度值，梯度代表了在(w,b)点出函数增长最快的方向，梯度的负方向就是函数值下降最快的方向，由此来寻找局部最小值，即梯度下降法。

* 函数的梯度给出了函数增长最快的方向，反之，梯度的负方向就是下降最快的方向。

* Twitter:Michale_ninlsen  http://neuralnetworksanddeeplearning.com/

* 知乎：张小磊

* B站评论中的一篇文章https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit


----

MLP 与神经网络区别



反向传播算法