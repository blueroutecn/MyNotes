# 机器学习部分概念整理

------

## 最小二乘法


#### 基本

最小二乘法（又称最小平方法）是一种数学优化技术。它通过**最小化误差的平方和寻找数据的最佳函数匹配**。  即令预测值Xw与数据集中观察到的值y两者之差的平方和尽量降到最小。写成数学 公式，即是要解决以下形式的问题

$$ min||X_w - y||^2$$


* 利用最小二乘法可以简便地**求得未知的数据**，并使得这些求得的数据与实际数据之间**误差的平方和为最小**

* 最小二乘法还可用于**曲线拟合**

#### 参考

* [scikit- Generalized Linear Models-1.1-1.1.1](http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)

## 梯度下降

#### 基本

* 梯度下降法，是当今最流行的优化（optimization）算法(最小二乘法也是一种优化算法)，亦是至今最常用的优化神经网络的方法。

* 在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。

* 梯度下降法通过多次迭代，并在每一步中最小化成本函数（cost function）来估计模型的参数（weights）。

#### 参考

* [ML之梯度下降算法](http://kissg.me/2017/07/23/gradient-descent/)

* [深度解读最流行的优化算法：梯度下降](https://www.jiqizhixin.com/articles/2016-11-21-4)

* [https://www.cnblogs.com/pinard/p/5970503.html](梯度下降小结)

## 学习率


#### 基本

* 深度学习模型通常由随机梯度下降算法进行训练。随机梯度下降算法有许多变形：例如 Adam、RMSProp、Adagrad 等等。这些算法都需要你设置**学习率**。学习率决定了在**一个小批量（mini-batch）中权重在梯度方向要移动多远**。

![](http://ouzh4pejg.bkt.clouddn.com/learning-rate.png)


* 设置学习率:

    * 训练应当从**相对较大的学习率**开始。这是因为在开始时，初始的随机权重远离最优值。


    * 在实践中,一般通过不同的尝试来设置学习率。可以先把学习速率设置为0.01，然后观察training cost的走向，如果cost在减小，那你可以逐步地调大学习速率，试试0.1，1.0….如果cost在增大，那就得减小学习速率，试试0.001，0.0001….经过一番尝试之后，你可以大概确定学习速率的合适的值（From:http://blog.csdn.net/u012162613/article/details/44265967  这篇文章还有详细的描述，具体查看吧）。
    
    
* 如下岭回归的例子中，alpha就是设置的**学习率**

```python
# FROM: http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression

>>> from sklearn import linear_model
>>> reg = linear_model.Ridge (alpha = .5)
>>> reg.fit ([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) 
Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,
      normalize=False, random_state=None, solver='auto', tol=0.001)
>>> reg.coef_
array([ 0.34545455,  0.34545455])
>>> reg.intercept_ 
0.13636...

```
#### 参考

* [如何估算神经网络的最优学习率](https://www.jiqizhixin.com/articles/2017-11-17-2)

* [http://blog.csdn.net/u012162613/article/details/44265967](http://blog.csdn.net/u012162613/article/details/44265967)

## 正则项系数

http://blog.csdn.net/u012162613/article/details/44265967

## Mini-batch size

* mini-batch size若设置为100，则是将100个样本的梯度求均值，替代online learning方法中单个样本的梯度值

http://blog.csdn.net/u012162613/article/details/44265967


## 普通最小二乘法sklearn.linear_model.LinearRegression

```python
from sklearn import	linear_model

clf	=	linear_model.LinearRegression()
# Train the model
clf.fit	([[0,	0],	[1,	1],	[2,	2]],	[0,	1,	2]) 
# The coefficient
print clf.coef_ 
# The instercept
print intercept_ 
```

## 岭回归sklearn.linear_model.RidgeRegression

#### 基本

* 岭回归引入了一种**对系数大小进行惩罚的措施**，来解决普通最小二乘可 能遇到的某些问题。岭回归最小化带有惩罚项的残差平方和：

$$ min||X_w - y||^2 + α||w||^2$$

* α是一个复杂的参数，用以**控制系数的缩减量**，该值越大,**系数缩减得越多，因而会对共线性更加鲁棒**。	

```python
>>>	from sklearn	import	linear_model 
>>>	clf	=	linear_model.Ridge	(alpha	=	.5) 
>>>	clf.fit	([[0,	0],	[0,	0],	[1,	1]],	[0,	.1,	1])	
Ridge(alpha=0.5,	copy_X=True,	fit_intercept=True,	max_iter=None, normalize=False,	random_state=None,	solver='auto',	tol=0.001) 
>>>	clf.coef_ array([	0.34545455,		0.34545455]) >>>	clf.intercept_	0.13636...


```
